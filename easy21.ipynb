{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy 21: A practical Introduction to RL\n",
    "\n",
    "This resource is intended to be a good place for beginners to get started with RL in a practical way and can be seen as partial summary of [RL course by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html).\n",
    "To obtain more information about the concepts here discussed please check the course!\n",
    "\n",
    "We'll implement the tasks described at the [Easy 21 assignment](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf).\n",
    "\n",
    "Special thanks to:\n",
    "  * David Silver, DeepMind and all involved for the great RL course;\n",
    "  * [AnalogRL implementation's of easy21](https://github.com/analog-rl/Easy21) which had great graphs including the\n",
    "  gifs you see in this notebook.\n",
    "\n",
    "> OBS: do you have suggestions? Found something wrong or want to improve some section? Feel free to open an issue or to make a PR :)!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Be familiar with Machine Learning concepts;\n",
    "* Be familiar with Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will be discussed?\n",
    "\n",
    "    1. Introduction to Reinforcement Learning (RL)\n",
    "    2. Learn as you go: solving Easy21 assigment\n",
    "       a) Implementing the Environment\n",
    "       b) Monte Carlo Policy Evaluation\n",
    "       c) Monte Carlo Policy Control\n",
    "       d) Sarsa lambda Policy Control\n",
    "       e) Linear Function approximation\n",
    "    3. What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "Reinforcement Learning is an area of Machine Learning that tries to understand the optimal way to make decisions.\n",
    "\n",
    "![](http://web.stanford.edu/class/cs234/images/header2.png)\n",
    "\n",
    "The way it basically works is there's an **agent** and an **environment**. The agent sees an observation (**state**) and possibily a **reward** (scalar feedback signal) and based on that it chooses an **action** that will affect the environment resulting in a new state and maybe a reward.\n",
    "\n",
    "We want to know what actions (decisions) we should make in a way to maximise the total future reward.\n",
    "\n",
    "A RL agent may include one or more:\n",
    "    \n",
    "  * **Policy**: agent's behaviour function. Given an state the policy outputs which action should the\n",
    "      agent choose. Policy(state) -> action.\n",
    "\n",
    "  * **Value function**: the expected future reward. This function tell us how good is to be at a certain state.  \n",
    "      There are basically two types of value function:\n",
    "      * **V(s)** -> how good is to be at state s (using a certain policy)?\n",
    "      * **Q(s, a)** -> given that we are at state s how good is to choose the action a (using a certain policy)?\n",
    "\n",
    "  * **Model**: an agent can have it's own representation of the environemnt. This involves defining transitions\n",
    "    (s1, a1) -> s2 and rewards.\n",
    "\n",
    "Two other important concepts are:\n",
    "\n",
    "* **return**: represented by **G**. The return at a given time is the sum of all rewards from that time on, times a discount factor:\n",
    "\n",
    "```\n",
    "Gt = sum((discount_factor ** i) * ri for i, ri in enumerate(rewards[t:]))\n",
    "```\n",
    "\n",
    "We can then define the value function of a given state as the expected return from that state on.\n",
    "\n",
    "* **episode**: sequence of states, actions and rewards (s1, a1, r1, ..., sn, an, rn) generated\n",
    "  by exploring the environment.\n",
    "\n",
    "---\n",
    "\n",
    "There are many more concepts, but for now is already clear that many types of RL agents are possible (with/no value function, whith/no model, ...) and the same for the environment.\n",
    "\n",
    "For now let's focus on the questions that are more important for our task:\n",
    "\n",
    " * How to evaluate a policy? In other words, given a policy how to know the value function?\n",
    " * How can we get to the best possible policy? **This solves the problem!!!**\n",
    "\n",
    "There are a lot of different algorithms to answer these questions, the idea is that in this exercise we'll see and implement some of them.\n",
    "\n",
    "One thing you may have noticed is that RL is very different than ML:\n",
    "\n",
    "  * no supervisor, we use the reward signal to learn;\n",
    "  * feedback is not instantaneous;\n",
    "  * time matters, we're dealing with sequential data;\n",
    "  * agent's actions affect the next data it receives.\n",
    "\n",
    "You may have a lot of questions as we go, this is completelly normal given the amount of new concepts, so don't worry and if you want to learn more and maybe get some answers go to the last section **3. Next steps** where you'll find more complete resourcers and full courses.\n",
    "\n",
    "---\n",
    "\n",
    "Want to know how to get started on RL? Check [Karpathy's tips on his blog post](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learn as you go: solving Easy21 assigment\n",
    "\n",
    "**Rules of the [Easy 21 game]([Easy 21 assignment](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf)**\n",
    "\n",
    "* The game is played with an infinite deck of cards (i.e. cards are sampled with replacement)\n",
    "* Each draw from the deck results in a value between 1 and 10 (uniformly distributed) with a colour of red (probability 1/3) or black (probability 2/3)\n",
    "* There are no aces or picture (face) cards in this game\n",
    "* At the start of the game both the player and the dealer draw one black card (fully observed)\n",
    "* Each turn the player may either stick or hit\n",
    "* If the player hits then she draws another card from the deck\n",
    "* If the player sticks she receives no further cards\n",
    "* The values of the player’s cards are added (black cards) or subtracted (red cards)\n",
    "* If the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)\n",
    "* If the player sticks then the dealer starts taking turns. The dealer always sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1), lose (reward -1), or draw (reward 0) – is the player with the largest sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before get started: Dependencies + Code used for plots\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "First let's import the python libraries needed to run all the code present in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Copyright [2017] [Marianne Linhares Monteiro, github: @mari-linhares]\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "'''\n",
    "# for vectors manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for plotting stuff\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "# visualize plots in the jupyter notebook\n",
    "# check more https://goo.gl/U3Ai8R\n",
    "%matplotlib inline\n",
    "\n",
    "# for generating random values\n",
    "import random\n",
    "\n",
    "# for representing things like card value or colors\n",
    "from enum import Enum  \n",
    "\n",
    "# for copying python objects\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot code**\n",
    "\n",
    "The code below is used to generate the different plots you'll se in the following sections. No need to understand it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_value_function(agent, title='Value Function', generate_gif=False, train_steps=None):\n",
    "    '''\n",
    "    Plots a value function as a surface plot, like in: https://goo.gl/aF2doj\n",
    "\n",
    "    You can choose between just plotting the graph for the value function\n",
    "    which is the default behaviour (generate_gif=False) or to train the agent\n",
    "    a couple of times and save the frames in a gif as you train.\n",
    "\n",
    "    args:\n",
    "        agent.\n",
    "        title (string): plot title.\n",
    "        generate_gif (boolean): if want to save plots as a gif.\n",
    "        train_steps: if is not None and generate_gif = True, then will use this\n",
    "                     value as the number of steps to train the model at each frame.\n",
    "    '''\n",
    "    # you can change this values to change the size of the graph\n",
    "    fig = plt.figure(title, figsize=(10, 5))\n",
    "    # explanation about this line: https://goo.gl/LH5E7i\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    V = agent.get_value_function()\n",
    "    \n",
    "    if generate_gif:\n",
    "        print 'gif will be saved as %s' % title\n",
    "    \n",
    "    def plot_frame(ax):\n",
    "        # min value allowed accordingly with the documentation is 1\n",
    "        # we're getting the max value from V dimensions\n",
    "        min_x = 1\n",
    "        max_x = V.shape[0]\n",
    "        min_y = 1\n",
    "        max_y = V.shape[1]\n",
    "\n",
    "        # creates a sequence from min to max\n",
    "        x_range = np.arange(min_x, max_x)\n",
    "        y_range = np.arange(min_y, max_y)\n",
    "\n",
    "        # creates a grid representation of x_range and y_range\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "        # get value function for X and Y values\n",
    "        def get_stat_val(x, y):\n",
    "            return V[x, y]\n",
    "        Z = get_stat_val(X, Y)\n",
    "\n",
    "        # creates a surface to be ploted\n",
    "        # check documentation for details: https://goo.gl/etEhPP\n",
    "        ax.set_xlabel('Dealer Showing')\n",
    "        ax.set_ylabel('Player Sum')\n",
    "        ax.set_zlabel('Value')\n",
    "        return ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, \n",
    "                               linewidth=0, antialiased=False)\n",
    "\n",
    "    def animate(frame):\n",
    "        # clear the plot and create a new surface\n",
    "        ax.clear()\n",
    "        surf = plot_frame(ax)\n",
    "        # if we're going to generate a gif we need to train a couple of times\n",
    "        if generate_gif:\n",
    "            i = agent.iterations\n",
    "            # cool math to increase number of steps as we go\n",
    "            # this approach generates a cool gif, but feel free to change this value\n",
    "            if train_steps is None:\n",
    "                step_size = int(min(max(1, agent.iterations), 2 ** 16))\n",
    "            else:\n",
    "                step_size = train_steps\n",
    "\n",
    "            agent.train(step_size)\n",
    "            plt.title('%s MC score: %s frame: %s' % (title, float(agent.wins)/agent.iterations*100, frame))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        return surf\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, 32, repeat=False)\n",
    "\n",
    "    # requires gif writer\n",
    "    if generate_gif:\n",
    "        ani.save(title + '.gif', writer='imagemagick', fps=3)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_error_vs_episode(sqrt_error, lambdas, train_steps=1000000, eval_steps=1000,\n",
    "                          title='SQRT error VS episode number', save_as_file=False):\n",
    "    '''\n",
    "        Given the sqrt error between sarsa(lambda) for multiple lambdas and\n",
    "        an already trained MC control model this function plots a\n",
    "        graph: sqrt error VS episode number.\n",
    "        \n",
    "        Args:\n",
    "            sqrt_error (tensor): multiD tensor.\n",
    "            lambdas (tensor): 1D tensor.\n",
    "            train_steps (int): number the total steps used to train the models.\n",
    "            eval_steps (int): train_steps/eval_steps is the number of time the\n",
    "                              errors were calculated while training.\n",
    "            save_as_file (boolean).\n",
    "    '''\n",
    "    # avoid zero division\n",
    "    assert eval_steps != 0\n",
    "    x_range = np.arange(0, train_steps, eval_steps)\n",
    "    \n",
    "    # assert that the inputs are correct\n",
    "    assert len(sqrt_error) == len(lambdas)\n",
    "    for e in sqrt_error:\n",
    "        assert len(list(x_range)) == len(e)\n",
    "    \n",
    "    # create plot\n",
    "    fig = plt.figure(title, figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for i in xrange(len(sqrt_error)-1, -1, -1):\n",
    "        ax.plot(x_range, sqrt_error[i], label='lambda %.2f' % lambdas[i])\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    if save_as_file:\n",
    "        plt.savefig(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_vs_lambda(sqrt_error, lambdas, title='SQRT error vs lambda', save_as_file=False):\n",
    "    '''\n",
    "        Given the sqrt error between sarsa(lambda) for multiple lambdas and\n",
    "        an already trainedMC Control ths function plots a graph:\n",
    "        sqrt error VS lambda.\n",
    "        \n",
    "        Args:\n",
    "            sqrt_error (tensor): multiD tensor.\n",
    "            lambdas (tensor): 1D tensor.\n",
    "            title (string): Plot title.\n",
    "            save_as_file (boolean).\n",
    "        \n",
    "        The srt_error 1D length must be equal to the lambdas length.\n",
    "    '''\n",
    "    \n",
    "    # assert input is correct\n",
    "    assert len(sqrt_error) == len(lambdas)\n",
    " \n",
    "    # create plot\n",
    "    fig = plt.figure(title, figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Y are the last values found at sqrt_error\n",
    "    y = [s[-1] for s in sqrt_error]\n",
    "    ax.plot(lambdas, y)\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    if save_as_file:\n",
    "        plt.savefig(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Implementing the Environment\n",
    "\n",
    "You should write an environment that implements the game Easy21.\n",
    "\n",
    "Specifically, write a function, named step, which takes as input a state *s* (dealer’s first card 1–10 and the player’s sum 1–21), and an action *a* (hit or stick), and returns a sample of the next state *s* (which may be terminal if the game is finished) and a reward *r*.\n",
    "\n",
    "\n",
    "We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. You should treat the dealer’s moves as part of the environment, i.e. calling step with a stick action will play out the dealer’s cards and return the final reward and terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deck\n",
    "\n",
    "* The game is played with an infinite deck of cards (i.e. cards are sampled with replacement)\n",
    "* Each draw from the deck results in a value between 1 and 10 (uniformly distributed) with a colour of red (probability 1/3) or black (probability 2/3)\n",
    "* There are no aces or picture (face) cards in this game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Color(Enum):\n",
    "    RED = 0\n",
    "    BLACK = 1\n",
    "\n",
    "\n",
    "class Card(object):\n",
    "    def __init__(self, color=None):\n",
    "        self.value = self._get_random_value()\n",
    "        if color == Color.BLACK or color == Color.RED:\n",
    "            self.color = color\n",
    "        else:\n",
    "            self.color = self._get_random_color()\n",
    "\n",
    "    def _get_random_value(self):\n",
    "        '''\n",
    "            Generates integers between 1 and 10.\n",
    "        '''\n",
    "        return random.randint(1, 10)\n",
    "    \n",
    "    def _get_random_color(self):\n",
    "        '''\n",
    "            Generates random colors.\n",
    "            Color.RED with 1/3 and Color.BLACK with 2/3 probability.\n",
    "        '''\n",
    "        random_number = random.random()\n",
    "        if random_number <= 1/3.0:\n",
    "            return Color.RED\n",
    "        else:\n",
    "            return Color.BLACK\n",
    "\n",
    "\n",
    "class Deck(object):  \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def take_card(self, color=None):\n",
    "        return Card(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State (observation) and Action\n",
    "\n",
    "The state is defined by the dealers first card (which I called *dealer_sum*) and the agent cards (which I called *agent_sum*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, dealer_sum=0, agent_sum=0, is_terminal=False):\n",
    "        self.dealer_sum = dealer_sum\n",
    "        self.agent_sum = agent_sum\n",
    "        self.is_terminal = is_terminal\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    STICK = 0\n",
    "    HIT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealer\n",
    "\n",
    "* If the player sticks then the dealer starts taking turns. The dealer always sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1), lose (reward -1), or draw (reward 0) – is the player with the largest sum.\n",
    "\n",
    "The instruction above basically describe the dealer's policy, let's define a method policy that all players should implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Player(object):\n",
    "    '''This is a general class for a player of Easy21.'''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def policy(self, s):\n",
    "        '''Given the current state choose the next action.'''\n",
    "        return Action.HIT\n",
    "\n",
    "\n",
    "class Dealer(Player):\n",
    "    def __init__(self):\n",
    "        Player.__init__(self)\n",
    "\n",
    "    def policy(self, s):\n",
    "        '''Dealers policy as described in the assigment.'''\n",
    "        if s.dealer_sum >= 17:\n",
    "            return Action.STICK\n",
    "        else:\n",
    "            return Action.HIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment (Game)\n",
    "\n",
    "* At the start of the game both the player and the dealer draw one black card (fully observed)\n",
    "* Each turn the player may either stick or hit\n",
    "* If the player hits then she draws another card from the deck\n",
    "* If the player sticks she receives no further cards\n",
    "* The values of the player’s cards are added (black cards) or subtracted (red cards)\n",
    "* If the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    '''An environment for the game Easy21.'''\n",
    "    def __init__(self):\n",
    "        # the environment includes the dealer and the deck\n",
    "        self.dealer = Dealer()\n",
    "        self.deck = Deck()\n",
    "        \n",
    "        self.agent_max_value = 21  # max value an agent can get during the game\n",
    "        self.dealer_max_value = 10  # max value the dealer can get duting the game\n",
    "        self.actions_count = 2  # number of possible actions\n",
    "    \n",
    "    def check_bust(self, player_sum):\n",
    "        return player_sum <= 1 or player_sum > 21\n",
    "    \n",
    "    def generate_reward_bust(self, s):\n",
    "        if s.agent_sum > s.dealer_sum:\n",
    "            return 1\n",
    "        elif s.agent_sum == s.dealer_sum:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def take_card(self, card_color=None):\n",
    "        '''Returns a card from the deck.'''\n",
    "        Card = self.deck.take_card(card_color)\n",
    "        if Card.color == Color.BLACK:\n",
    "            return Card.value\n",
    "        else:\n",
    "            return -1 * Card.value\n",
    "\n",
    "    def dealer_turn(self, s):\n",
    "        '''A full implementation of the dealer turn.\n",
    "        \n",
    "           The dealer turn starts when the agent sticks and\n",
    "           ends when the dealer action is busted or action = sticks.\n",
    "        '''\n",
    "        action = None\n",
    "        while not s.is_terminal and action != Action.STICK:\n",
    "            action = self.dealer.policy(s)\n",
    "            if action == Action.HIT:\n",
    "                s.dealer_sum += self.take_card() \n",
    "            s.is_terminal = self.check_bust(s.dealer_sum)\n",
    "        return s\n",
    "    \n",
    "    def initial_state(self):\n",
    "        '''In the beginning both the agent and the dealer take a card.'''\n",
    "        return State(self.take_card(Color.BLACK), self.take_card(Color.BLACK))\n",
    "\n",
    "    def step(self, s, a):\n",
    "        '''\n",
    "            Given a state and an action return the next state.\n",
    "            \n",
    "            Args:\n",
    "                s (State): current state\n",
    "                a (Action): action chosen by player\n",
    "            return:\n",
    "                next_s (State): next state\n",
    "                r (Integer): reward [-1, 0, 1]\n",
    "        '''\n",
    "        \n",
    "        # initially there's no reward and the next_s is equal to the\n",
    "        # current state\n",
    "        r = 0\n",
    "        next_s = copy.copy(s)\n",
    "\n",
    "        # if the player sticks then it's dealer turn\n",
    "        if a == Action.STICK:\n",
    "            next_s = self.dealer_turn(s)\n",
    "            if next_s.is_terminal:\n",
    "                r = 1\n",
    "            else:\n",
    "                next_s.is_terminal = True\n",
    "                r = self.generate_reward_bust(next_s)       \n",
    "        else:\n",
    "            next_s.agent_sum += self.take_card(self.deck)\n",
    "            next_s.is_terminal = self.check_bust(next_s.agent_sum)\n",
    "\n",
    "            # if end of the game then player lost: reward = -1\n",
    "            if next_s.is_terminal:\n",
    "                r = -1\n",
    "        \n",
    "        # print next_s.dealer_sum, next_s.agent_sum, next_s.is_terminal\n",
    "        return next_s, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of the environment that the agent can interact with by calling the function **step(s, a)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "environment = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) Monte Carlo Policy Evaluation (MCPE)\n",
    "\n",
    "Although the Easy21 exercise doesn't ask for the MCPE implementention, this will give us a base to start working on the following algorithms. MCPE answers the question *\"How to evaluate a policy?\"*, in other words how to estimate the value function. This problem is called **prediction**.\n",
    "\n",
    "Monte Carlo Policy Evaluation (MCPE):\n",
    "\n",
    "* works only for \"episodic\" scenariums. It learns from episodes and all episodes must terminate.\n",
    "* estimates the value function of a policy X from episodes of experience following policy X.\n",
    "* is model-free: no knowledge of MDP transitions / rewards.\n",
    "* uses the simplest possible idea: value = mean return.\n",
    "\n",
    "The MCPE pseudo-code is:\n",
    "```\n",
    "for a big number of steps:\n",
    "    Run one episode and store the states you've visited and the rewards you got.\n",
    "    for every time-step t:\n",
    "        Increment counter: N(s) ← N(s) + 1\n",
    "        Increment total return: G_s(s) ← G_s(s) + Gt\n",
    "        Value is estimated by mean return V(s) = S(s)/N(s)\n",
    "\n",
    "The value function will get close to the real value function as N(s) → ∞\n",
    "```\n",
    "\n",
    "A question you may have is: we know how to evaluate a policy but what policy will be used?\n",
    "Let's use a very simple policy, the same policy of the dealer! if the sum of the agent's cards is > 17\n",
    "we stick otherwise we hit.\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** General Agent **\n",
    "\n",
    "Before going into details about Monte Carlo Policy Evaluation, let's implement a General class Agent, this will help us reuse a lot of code and understand better the agent's interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(Player):  \n",
    "    def __init__(self, environment, No=100, discount_factor=1): \n",
    "        \n",
    "        # Player is a superclass, which means an agent implements a policy\n",
    "        Player.__init__(self)\n",
    "        \n",
    "        # easy21 environment\n",
    "        self.env = environment\n",
    "        \n",
    "        # we can tune these parameters\n",
    "        # don't worry about this for now\n",
    "        self.No = No\n",
    "        self.disc_factor = discount_factor\n",
    "        \n",
    "        # V(s) is the state value function. How good is to be at state s?\n",
    "        self.V = np.zeros([self.env.dealer_max_value + 1, self.env.agent_max_value + 1])\n",
    "        \n",
    "        # this will be used to we keep track of the agent's score\n",
    "        # score is a simple metric to check if the agent is getting better rewards over time\n",
    "        # score = (self.wins / self.iterations) * 100 %\n",
    "        self.wins = 0.0\n",
    "        self.iterations = 0.0\n",
    "  \n",
    "    def get_clear_tensor(self):\n",
    "        '''\n",
    "            This is just a helper function. Not important.\n",
    "\n",
    "            Returns a tensor with zeros with the correct given shape for Q.\n",
    "            By default this is (max possible dealer sum, max possible agent sum, number of actions)\n",
    "        '''\n",
    "        return np.zeros((self.env.dealer_max_value + 1,\n",
    "                         self.env.agent_max_value + 1, \n",
    "                         self.env.actions_count))\n",
    " \n",
    "    def choose_random_action(self):\n",
    "        '''\n",
    "           Once we try to actual get the best policy possible \n",
    "           we will act randomly sometimes.\n",
    "        '''\n",
    "        return Action.HIT if random.random() <= 0.5 else Action.STICK\n",
    "\n",
    "    def choose_best_action(self, s):\n",
    "        '''Returns the best action possible in state s.'''\n",
    "        return Action.HIT\n",
    "    \n",
    "    def get_max_action(self, s):\n",
    "        '''Returns the maxQ(s, a) between all actions.'''\n",
    "        return 0.0\n",
    "\n",
    "    def get_value_function(self):\n",
    "        '''Get best value function in the moment.'''\n",
    "        for i in xrange(1, self.env.dealer_max_value + 1):\n",
    "            for j in xrange(1, self.env.agent_max_value + 1):\n",
    "                s = State(i, j)\n",
    "                self.V[i][j] = self.get_max_action(s)\n",
    "        return self.V\n",
    "\n",
    "    def train(self, steps):\n",
    "        '''Train an agent for a certain number of steps.\n",
    "        \n",
    "           Args:\n",
    "               steps (int): number of episodes to run.\n",
    "           Returns:\n",
    "               value function.\n",
    "        '''\n",
    "        for e in xrange(steps):\n",
    "            # do something\n",
    "            pass\n",
    "        return self.get_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Monte Carlo Evaluation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MCAgentEvaluation(Agent):\n",
    "    \n",
    "    def __init__(self, environment, No=100, discount_factor=1):\n",
    "        Agent.__init__(self, environment, No, discount_factor)\n",
    "\n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s, a) is the number of times that action a has been selected from state s.\n",
    "        self.N = self.get_clear_tensor()\n",
    "        \n",
    "        # G_s(s) is the sum of all returns we got using this state\n",
    "        self.G_s = np.zeros([self.env.dealer_max_value + 1, self.env.agent_max_value + 1])\n",
    "  \n",
    "    def get_value_function(self):\n",
    "        return self.V\n",
    "    \n",
    "    def predict(self, episode):\n",
    "        '''Given an episode. Improve the value function approximation.'''\n",
    "        j = 0\n",
    "        for s, a, _ in episode:\n",
    "            d_sum = s.dealer_sum\n",
    "            a_sum = s.agent_sum\n",
    "            Gt = sum([x[2]*(self.disc_factor**i) for i,x in enumerate(episode[j:])])\n",
    "            self.G_s[d_sum][a_sum] += Gt\n",
    "            self.V[d_sum][a_sum] = self.G_s[d_sum][a_sum] / sum(self.N[s.dealer_sum, s.agent_sum, :])\n",
    "            j += 1\n",
    "    \n",
    "    def policy(self, s):\n",
    "        '''Lets act like the dealer.'''\n",
    "        if s.agent_sum >= 17:\n",
    "            action = Action.STICK\n",
    "        else:\n",
    "            action = Action.HIT\n",
    "        \n",
    "        self.N[s.dealer_sum][s.agent_sum][action.value] += 1\n",
    "        return action\n",
    "    \n",
    "    def train(self, steps):\n",
    "        for e in xrange(steps):\n",
    "            episode = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            s = self.env.initial_state()\n",
    "            \n",
    "            # while game has not ended\n",
    "            while not s.is_terminal:\n",
    "                \n",
    "                # get action with epsilon greedy policy\n",
    "                a = self.policy(s)\n",
    "                \n",
    "                # execute action\n",
    "                next_s, r = self.env.step(copy.copy(s), a)\n",
    "                \n",
    "                # store action state and reward\n",
    "                episode.append((s, a, r))\n",
    "                \n",
    "                # update state\n",
    "                s = next_s\n",
    "\n",
    "            self.iterations += 1\n",
    "            if e % 10000 == 0 and e != 0:\n",
    "                print \"Episode: %d\" % e\n",
    "                \n",
    "            # Update value function accordingly\n",
    "            self.predict(episode) \n",
    "\n",
    "        return self.get_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_eval = MCAgentEvaluation(environment)\n",
    "agent_eval.train(100000)\n",
    "\n",
    "plot_value_function(agent_eval, title='Value function: acting like the dealer (MC policy evaluation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is the plot of the value function for every possible state (dealer sum or player sum) we can be at. What it says is basically is very good to have a big sum and act accordingly to this policy, but if you have a small sum not so good.\n",
    "\n",
    "Big value function (red) -> good state, Small value function (blue) -> bad state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating a gif of the evaluation over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncoment these lines to generate the gif you se below\n",
    "# agent_eval = MCAgentEvaluation(environment)\n",
    "# plot_value_function(agent_eval, title='Value function: acting like the dealer (MC policy evaluation)',\n",
    "#                    generate_gif=True, train_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/mcpe.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. c) Monte Carlo Policy Control (MCPC)\n",
    "\n",
    "All the following algorithms we'll see try to answer the question: *\"How can we get to the best possible policy?\"*\n",
    "\n",
    "Since we want the best policy we need to explore other possibilities every once in a while other than the best we know. This is actually the well know dilemma in RL: [exploration vs explotation dilemma](www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf).\n",
    "\n",
    "The E-greedy exploration which is one of the simplest ideas to ensure continual exploration and is defined by:\n",
    "\n",
    "```\n",
    "policy(a | st) = { probability 1 - et to choose the greedy action (the best we know, maxQ(st, a)),\n",
    "                   probability et choose random action\n",
    "                 }\n",
    "\n",
    "```\n",
    "\n",
    "We have our policy, let's check the pseudo-code for the MCPC. We improve our value function by sampling episodes and exploring the environment.\n",
    "\n",
    "```\n",
    "for a big number of steps:\n",
    "    Run one episode and store the states you've visited and the rewards you got.\n",
    "    for every time-step t:\n",
    "        Increment counter: N(St) ← N(St) + 1\n",
    "        error ← (Gt − Q(St, At))\n",
    "        Q(St, At) ← Q(St, At) + αt * error\n",
    "```\n",
    "\n",
    "For this exercise:\n",
    "\n",
    "* E-greedy exploration strategy with et = N0/(N0 + N(st)), where N0 = 100 is a constant, N(s) is the number of times that state s has been visited, and N(s, a) is the number of times that action a has been selected from state s. Feel free to choose an alternative value for N0, if it helps producing better results.\n",
    "* Initialise the value function to zero.\n",
    "* Use a time-varying scalar step-size of αt = 1/N(st, at) and an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MCAgentControl(Agent):\n",
    "    \n",
    "    def __init__(self, environment, No=100, discount_factor=1):\n",
    "        \n",
    "        Agent.__init__(self, environment, No, discount_factor)\n",
    "        \n",
    "        # Q(s, a) is the value function. How good is to choose action a being at state s?\n",
    "        self.Q = self.get_clear_tensor()\n",
    "        \n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s, a) is the number of times that action a has been selected from state s.\n",
    "        self.N = self.get_clear_tensor()\n",
    "        \n",
    "    def get_alpha(self, s, a):\n",
    "        '''αt = 1/N(st, at)'''\n",
    "        return 1.0/(self.N[s.dealer_sum][s.agent_sum][a.value])\n",
    "    \n",
    "    def get_e(self, s):\n",
    "        '''et = N0/(N0 + N(st))'''\n",
    "        return self.No/((self.No + sum(self.N[s.dealer_sum, s.agent_sum, :]) * 1.0))\n",
    "    \n",
    "    def get_max_action(self, s):\n",
    "        return np.max(self.Q[s.dealer_sum][s.agent_sum])\n",
    "    \n",
    "    def choose_best_action(self, s):\n",
    "        return Action.HIT if np.argmax(self.Q[s.dealer_sum][s.agent_sum]) == 1 else Action.STICK\n",
    "\n",
    "    def control(self, episode):\n",
    "        '''Given an episode. Improve the value function approximation towards Q*(s, a).'''\n",
    "        j = 0\n",
    "        for s, a, _ in episode:\n",
    "            d_sum = s.dealer_sum\n",
    "            a_sum = s.agent_sum\n",
    "            \n",
    "            Gt = sum([x[2]*(self.disc_factor**i) for i,x in enumerate(episode[j:])])\n",
    "            \n",
    "            self.N[d_sum][a_sum][a.value] += 1\n",
    "            \n",
    "            error = Gt - self.Q[d_sum][a_sum][a.value]\n",
    "            self.Q[d_sum][a_sum][a.value] += self.get_alpha(s, a) * error\n",
    "            \n",
    "            j += 1\n",
    "            \n",
    "    def policy(self, s):\n",
    "        r = random.random()\n",
    "        if r <= self.get_e(s):\n",
    "            action = self.choose_random_action()\n",
    "        else:\n",
    "            action = self.choose_best_action(s)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def train(self, steps):\n",
    "        for e in xrange(steps):\n",
    "            episode = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            s = self.env.initial_state()\n",
    "            \n",
    "            # while game has not ended\n",
    "            while not s.is_terminal:\n",
    "                \n",
    "                # get action with epsilon greedy policy\n",
    "                a = self.policy(s)\n",
    "                \n",
    "                # execute action\n",
    "                next_s, r = self.env.step(copy.copy(s), a)\n",
    "                \n",
    "                # store action state and reward\n",
    "                episode.append((s, a, r))\n",
    "                \n",
    "                # update state\n",
    "                s = next_s\n",
    "\n",
    "            if e % 10000 == 0 and self.iterations > 0:\n",
    "                print \"Episode: %d, score: %f\" % (e, (float(self.wins)/(self.iterations)*100.0))\n",
    "            \n",
    "            # update wins and iterations\n",
    "            self.iterations += 1\n",
    "            if r == 1:\n",
    "                self.wins += 1\n",
    "                \n",
    "            # Update Action value function accordingly\n",
    "            self.control(episode) \n",
    "\n",
    "        return self.get_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot value function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mc_agent = MCAgentControl(environment)\n",
    "mc_agent.train(1000000)\n",
    "\n",
    "plot_value_function(mc_agent, title=\"MC Control Value function No=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/mcpc.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncoment these lines to generate the gif you se below\n",
    "# mc_agent = MCAgentControl(environment)\n",
    "# plot_value_function(mc_agent, title='MC Control Value function No=100',\n",
    "#                     generate_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. d) Sarsa lambda Policy Control\n",
    "\n",
    "*Implement Sarsa(λ) in 21s. Initialise the value function to zero. Use the same\n",
    "step-size and exploration schedules as in the previous section.*\n",
    "\n",
    "\n",
    "Sarsa is another algorithm for policy control, for the MC control algorithm we had to\n",
    "finish the episode in order to update our value function, this is not true for Sarsa, now\n",
    "we can do online updates.\n",
    "\n",
    "In order to do this, instead of updating our value function accordingly with the future (return G)\n",
    "we'll update it based on the next state and on a **eligibility trace**. The idea is that the\n",
    "eligibility trace keeps track of how many times we've been on a state and how long has it been that\n",
    "we were at that state. [A little more about it here](https://www.quora.com/Whats-your-view-on-eligibility-traces-for-temporal-difference-learning-Is-it-just-a-credit-assignment-or-speed-up-trick).\n",
    "\n",
    "The eligibility trace is updated at every step, the algorithm to update it is below:\n",
    "\n",
    "```\n",
    "The current state is St.\n",
    "    \n",
    "    e_trace(St) = e_trace(St) * discout_factor * lambda + 1\n",
    "    For every other state S other than St:\n",
    "        e_trace(S) = e_trace(S) * discout_factor * lambda\n",
    "```\n",
    "![](http://cs.stanford.edu/people/karpathy/reinforcejs/img/traces.png)\n",
    "\n",
    "Now talking about Sarsa, the pseudo-code is:\n",
    "\n",
    "![](https://i.stack.imgur.com/TSr4m.png)\n",
    "\n",
    "\n",
    "> FWI: there are 2 versions of sarsa, a forward view that needs a full episode in order to update the value function and a backward view (the one we'll implement!) which uses the eligibility trace. These approaches\n",
    "are equivalents.\n",
    "\n",
    "\n",
    "This is a more complicated algorithm so take some time to read the theory and understand why it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SarsaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, environment, No=100, discount_factor=1, _lambda=1):\n",
    "        Agent.__init__(self, environment, No, discount_factor)\n",
    "      \n",
    "        # we can tune this parameter\n",
    "        self._lambda = _lambda\n",
    "        \n",
    "        # Eligibility Trace\n",
    "        self.E = self.get_clear_tensor()\n",
    "        \n",
    "        # Q(s, a) is the value function. How good is to choose action a being at state s?\n",
    "        self.Q = self.get_clear_tensor()\n",
    "        \n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s, a) is the number of times that action a has been selected from state s.\n",
    "        self.N = self.get_clear_tensor()\n",
    "    \n",
    "    def get_q(self, s, a):\n",
    "        return self.Q[s.dealer_sum][s.agent_sum][a.value]\n",
    "            \n",
    "    def get_alpha(self, s, a):\n",
    "        '''αt = 1/N(st, at)'''\n",
    "        return 1.0/(self.N[s.dealer_sum][s.agent_sum][a.value])\n",
    "    \n",
    "    def get_e(self, s):\n",
    "        '''et = N0/(N0 + N(st))'''\n",
    "        return self.No/((self.No + sum(self.N[s.dealer_sum, s.agent_sum, :]) * 1.0))\n",
    "    \n",
    "    def get_max_action(self, s):\n",
    "        return np.max(self.Q[s.dealer_sum][s.agent_sum])\n",
    "    \n",
    "    def choose_best_action(self, s):\n",
    "        return Action.HIT if np.argmax(self.Q[s.dealer_sum][s.agent_sum]) == 1 else Action.STICK\n",
    "                \n",
    "    def policy(self, s):\n",
    "        r = random.random()\n",
    "        if r <= self.get_e(s):\n",
    "            action = self.choose_random_action()\n",
    "        else:\n",
    "            action = self.choose_best_action(s)\n",
    "            \n",
    "        self.N[s.dealer_sum][s.agent_sum][action.value] += 1\n",
    "        return action\n",
    "\n",
    "    def train(self, steps):\n",
    "        for e in xrange(steps):\n",
    "            # clear eligibility trace\n",
    "            self.E = self.get_clear_tensor()\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            s = self.env.initial_state()\n",
    "\n",
    "            # choose a from s with epsilon greedy policy\n",
    "            a = self.policy(s)\n",
    "            next_a = a \n",
    "            \n",
    "            # while game has not ended\n",
    "            while not s.is_terminal:\n",
    "                \n",
    "                # execute action\n",
    "                next_s, r = self.env.step(copy.copy(s), a)\n",
    "              \n",
    "                q = self.get_q(s, a)\n",
    "                \n",
    "                if not next_s.is_terminal:\n",
    "                    # choose next action with epsilon greedy policy\n",
    "                    next_a = self.policy(next_s)\n",
    "                    q_next = self.get_q(next_s, next_a)\n",
    "                    delta = r + q_next - q\n",
    "                else:\n",
    "                    delta = r - q \n",
    "            \n",
    "                self.E[s.dealer_sum][s.agent_sum][a.value] += 1\n",
    "                alpha = self.get_alpha(s, a)\n",
    "                update_q = alpha * delta * self.E\n",
    "                self.Q += update_q\n",
    "                self.E *= (self.disc_factor * self._lambda)\n",
    "                \n",
    "                # update state and action\n",
    "                s = next_s\n",
    "                a = next_a\n",
    "\n",
    "            if e % 100000 == 0 and e != 0:\n",
    "                print \"Episode: %d, score: %f\" % (e, (float(self.wins)/self.iterations)*100)\n",
    "            \n",
    "            # update wins and iterations\n",
    "            self.iterations += 1\n",
    "            if r == 1:\n",
    "                self.wins += 1\n",
    "\n",
    "        return self.get_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarsa_agent = SarsaAgent(environment)\n",
    "sarsa_agent.train(1000000)\n",
    "\n",
    "plot_value_function(sarsa_agent, title=\"Sarsa No=100 lambda = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/sarsa.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "*Run the algorithm\n",
    "with parameter values λ ∈ {0, 0.1, 0.2, ..., 1}. Stop each run after 1000 episodes\n",
    "2\n",
    "and report the mean-squared error P\n",
    "s,a(Q(s, a) − Q∗\n",
    "(s, a))2 over all states s\n",
    "and actions a, comparing the true values Q∗\n",
    "(s, a) computed in the previous\n",
    "section with the estimated values Q(s, a) computed by Sarsa. Plot the meansquared\n",
    "error against λ. For λ = 0 and λ = 1 only, plot the learning curve of\n",
    "mean-squared error against episode number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = [e/10.0 for e in range(0, 11, 1)]\n",
    "print 'lambdas:', lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_sqr(q1, q2):\n",
    "    return np.sum(np.square(q1-q2))\n",
    "\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = 1000\n",
    "errors = []\n",
    "for i, l in enumerate(lambdas):\n",
    "    print 'Training Sarsa(%.1f)' % l\n",
    "    errors.append([])\n",
    "    sarsa_agent = SarsaAgent(environment, _lambda=l)\n",
    "    for j in xrange(TRAIN_STEPS/EVAL_STEPS):\n",
    "        sarsa_agent.train(EVAL_STEPS)\n",
    "        errors[i].append(mean_sqr(mc_agent.Q, sarsa_agent.Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_error_vs_episode(errors, lambdas, train_steps=TRAIN_STEPS, eval_steps=EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/sqrt_error_vs_steps_sarsa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_error_vs_lambda(errors, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/sqrt_error_vs_lambda_sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. e) Linear Function approximation\n",
    "\n",
    "Sometimes the number of states and actions is too big, and using a matrix or tensor to represent all these states and actions is not a scalable solution and maybe not even possible. Fortunatelly there's an easy solution, we can use a value function approximator that can represent a state and action as a low-dim vector, this approximator can be a linear regression, neural network, conv net,...\n",
    "\n",
    "For this exercise we'll use a simple linear approximator, since the update is very simple:\n",
    "    Update = step-size × prediction error × feature value\n",
    "    \n",
    "And now our value function will be given by: phi(s, a) * theta, where phi(s, a) is a approximate representation of this state and theta is our tensor of paremeters that we'll actually update.\n",
    "\n",
    "We now consider a simple value function approximator using coarse coding. Use a binary feature vector φ(s, a) with 3 x 6 x 2 = 36 features. Each binary feature\n",
    "has a value of 1 iff (s, a) lies within the cuboid of state-space corresponding to\n",
    "that feature, and the action corresponding to that feature. The cuboids have\n",
    "the following overlapping intervals:\n",
    "dealer(s) = {[1, 4], [4, 7], [7, 10]}\n",
    "player(s) = {[1, 6], [4, 9], [7, 12], [10, 15], [13, 18], [16, 21]}\n",
    "a = {hit, stick}\n",
    "where\n",
    "• dealer(s) is the value of the dealer’s first card (1–10)\n",
    "• sum(s) is the sum of the player’s cards (1–21)\n",
    "Repeat the Sarsa(λ) experiment from the previous section, but using linear\n",
    "value function approximation Q(s, a) = φ(s, a)\n",
    ">θ. Use a constant exploration\n",
    "of e = 0.05 and a constant step-size of 0.01. Plot the mean-squared error against\n",
    "λ. For λ = 0 and λ = 1 only, plot the learning curve of mean-squared error\n",
    "against episode number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearFunctionSarsaAgent(Agent):\n",
    "    \n",
    "    def __init__(self, environment, No=100, discount_factor=1, _lambda=1):    \n",
    "        Agent.__init__(self, environment, No, discount_factor)\n",
    "        \n",
    "        # we can tune these parameters\n",
    "        self._lambda = _lambda\n",
    "        self.number_of_parameters = 36\n",
    "        \n",
    "        # parameters are initialize randomly\n",
    "        self.theta = np.random.randn(self.number_of_parameters) * 0.1\n",
    "        \n",
    "        # eligibility trace\n",
    "        self.E = self.get_clear_tensor()\n",
    "  \n",
    "        # features\n",
    "        self.dealer_features = [[1, 4], [4, 7], [7, 10]]\n",
    "        self.agent_features = [[1, 6], [4, 9], [7, 12], [10, 15], [13, 18], [16, 21]]\n",
    "    \n",
    "    def get_clear_tensor(self):\n",
    "        '''\n",
    "            Returns a tensor with zeros with the correct shape.\n",
    "        '''\n",
    "        return np.zeros(self.number_of_parameters)\n",
    "\n",
    "    def get_q(self, s, a):\n",
    "        return np.dot(self.phi(s, a), self.theta)\n",
    "    \n",
    "    def get_all_q(self):\n",
    "        q = np.zeros((self.env.dealer_max_value + 1,\n",
    "                         self.env.agent_max_value + 1, \n",
    "                         self.env.actions_count))\n",
    "        \n",
    "        for i in xrange(1, self.env.dealer_max_value + 1):\n",
    "            for j in xrange(1, self.env.agent_max_value + 1):\n",
    "                for a in [Action.HIT, Action.STICK]:\n",
    "                    s = State(i, j)\n",
    "                    q[i, j, a.value] = self.get_q(s, a)\n",
    "    \n",
    "        return q\n",
    "\n",
    "    def phi(self, s, a):\n",
    "        d_sum = s.dealer_sum\n",
    "        a_sum = s.agent_sum\n",
    "        \n",
    "        features = np.zeros((3, 6, 2), dtype=np.int)\n",
    "        \n",
    "        d_features = np.array([x[0] <= d_sum <= x[1] for x in self.dealer_features])\n",
    "        a_features = np.array([x[0] <= a_sum <= x[1] for x in self.agent_features])\n",
    "        \n",
    "        for i in np.where(d_features):\n",
    "            for j in np.where(a_features):\n",
    "                features[i, j, a.value] = 1\n",
    "\n",
    "        return features.flatten()\n",
    "   \n",
    "    def get_alpha(self, s, a):\n",
    "        return 0.01\n",
    "       \n",
    "    def get_e(self, s):\n",
    "        return 0.05\n",
    "\n",
    "    def try_all_actions(self, s):\n",
    "        return [np.dot(self.phi(s, Action.STICK), self.theta), np.dot(self.phi(s, Action.HIT), self.theta)]\n",
    "    \n",
    "    def get_max_action(self, s):\n",
    "        return np.max(self.try_all_actions(s))\n",
    "    \n",
    "    def choose_best_action(self, s):\n",
    "        return Action.HIT if np.argmax(self.try_all_actions(s)) == 1 else Action.STICK\n",
    "    \n",
    "    def policy(self, s):\n",
    "        r = random.random()\n",
    "        if r <= self.get_e(s):\n",
    "            action = self.choose_random_action()\n",
    "        else:\n",
    "            action = self.choose_best_action(s)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self, steps):\n",
    "        for e in xrange(steps):\n",
    "            # clear eligibility trace\n",
    "            self.E = self.get_clear_tensor()\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            s = self.env.initial_state()\n",
    "\n",
    "            # choose a from s with epsilon greedy policy\n",
    "            a = self.policy(s)\n",
    "            next_a = a \n",
    "                 \n",
    "            # while game has not ended\n",
    "            while not s.is_terminal:\n",
    "                \n",
    "                # execute action\n",
    "                next_s, r = self.env.step(copy.copy(s), a)\n",
    "              \n",
    "                # get parameters that represent this state and action\n",
    "                phi = self.phi(s, a)\n",
    "                # get q(s, a)\n",
    "                q = self.get_q(s, a)\n",
    "                \n",
    "                if not next_s.is_terminal:\n",
    "                    # choose next action with epsilon greedy policy\n",
    "                    next_a = self.policy(next_s)\n",
    "                    q_next = self.get_q(next_s, next_a)\n",
    "                    delta = r + q_next - q\n",
    "                else:\n",
    "                    delta = r - q \n",
    "            \n",
    "                self.E += phi\n",
    "                alpha = self.get_alpha(s, a)\n",
    "                update_q = alpha * delta * self.E\n",
    "                self.theta += update_q\n",
    "                self.E *= (self.disc_factor * self._lambda)\n",
    "                \n",
    "                # update state and action\n",
    "                s = next_s\n",
    "                a = next_a\n",
    "\n",
    "            if e % 10000 == 0 and e != 0:\n",
    "                print \"Episode: %d, score: %f\" % (e, (float(self.wins)/self.iterations)*100)\n",
    "            \n",
    "            # update wins and iterations\n",
    "            self.iterations += 1\n",
    "            if r == 1:\n",
    "                self.wins += 1\n",
    "\n",
    "        return self.get_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarsa_agent = SarsaAgent(environment)\n",
    "# sarsa_agent.train(1000000)\n",
    "\n",
    "plot_value_function(sarsa_agent, title=\"Linear Function Sarsa No=100 lambda = 1\", generate_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/linear_function_sarsa.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_value_function(sarsa_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_sqr(q1, q2):\n",
    "    return np.sum(np.square(q1-q2))\n",
    "\n",
    "EVAL_STEPS = 1000\n",
    "TEST_STEPS = 10000\n",
    "error = []\n",
    "for i, l in enumerate(lambdas):\n",
    "    print 'Training Sarsa(%f)' % l\n",
    "    error.append([])\n",
    "    sarsa_agent = LinearFunctionSarsaAgent(environment, _lambda=l)\n",
    "    for j in xrange(TRAIN_STEPS/EVAL_STEPS):\n",
    "        sarsa_agent.train(TRAIN_STEPS/EVAL_STEPS)\n",
    "        error[i].append(mean_sqr(mc_agent.Q.q, sarsa_agent.get_all_q()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_lambdas(error, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What's next?\n",
    "\n",
    "* [RL Course by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\n",
    "* [How do I learn reinforcement learning?](https://www.quora.com/How-do-I-learn-reinforcement-learning)\n",
    "* [CS234: Reinforcement Learning](http://web.stanford.edu/class/cs234/index.html)\n",
    "* [Open AI gym](https://gym.openai.com/envs)\n",
    "* [Denny Britz RL github repository](https://github.com/dennybritz/reinforcement-learning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
